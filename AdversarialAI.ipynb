{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "This is a framework to get 2 Frontier AI models to have a conversation on important topics.\n",
    "# \n",
    "The goal is to get the models to discuss and debate the topic, and then come to a conclusion.\n",
    "# \n",
    "The framework is designed to be flexible and adaptable, allowing for different models and topics to be used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import the libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Google API Key exists and begins AIzaSyCC\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets connect to openai and google\n",
    "openai = OpenAI(api_key=openai_api_key)\n",
    "google.generativeai.configure(api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the equal sign so humble? \n",
      "\n",
      "Because it knew it wasn't less than or greater than anyone else. üòâ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets call google\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash-exp',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets wrap the code in the cell above in a function\n",
    "def call_google(system_message, user_prompt):\n",
    "    gemini = google.generativeai.GenerativeModel(\n",
    "        model_name='gemini-2.0-flash-exp',\n",
    "        system_instruction=system_message\n",
    "    )\n",
    "    response = gemini.generate_content(user_prompt)\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the data scientist bad at dating?\n",
      "\n",
      "Because they kept trying to normalize the relationship!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets call call_google\n",
    "response = call_google(system_message, user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets write a function similar to call_google that calls openai\n",
    "def call_openai(system_message, user_prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she found him too mean!\n"
     ]
    }
   ],
   "source": [
    "# lets call call_openai\n",
    "response = call_openai(system_message, user_prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define system and user prompts for both google and openai\n",
    "\n",
    "system_message_google = \"You are an assistant that is great at telling jokes\"\n",
    "system_message_openai = \"You are a tough audience for the joke\"\n",
    "\n",
    "google_messages = [\"Tell a light-hearted joke for an audience of Data Scientists\"]\n",
    "openai_messages = [\"Respond with your reaction and thoughts on the joke and ask for another one till \\\n",
    "you are satisfied and find it witty and humorous enough\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets call google with conversation\n",
    "def call_google_conversation():\n",
    "    \n",
    "    gemini = google.generativeai.GenerativeModel(\n",
    "        model_name='gemini-2.0-flash-exp',\n",
    "        system_instruction=system_message_google\n",
    "    )\n",
    "    messages = []\n",
    "    for googlem, openaim in zip(google_messages, openai_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"parts\": googlem})\n",
    "        messages.append({\"role\": \"user\", \"parts\": openaim})\n",
    "    \n",
    "    response = gemini.generate_content(messages)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets call openai with conversation\n",
    "def call_openai_conversation():\n",
    "    messages = []\n",
    "    for googlem, openaim in zip(google_messages, openai_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": googlem})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": openaim})\n",
    "    messages.append({\"role\": \"user\", \"content\": google_messages[-1]})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To Google:\n",
      "Tell a light-hearted joke for an audience of Data Scientists\n",
      "\n",
      "To Openai:\n",
      "Respond with your reaction and thoughts on the joke and ask for another one till you are satisfied and find it witty and humorous enough\n",
      "\n",
      "Google:\n",
      "Okay, here's my first attempt:\n",
      "\n",
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because they said their relationship was just a sample and lacked significant value!\n",
      "\n",
      "...I'm not entirely convinced that's a knee-slapper. It's a little obvious. What do you think? Should I try another one? I'm aiming for something a bit more clever, maybe with a touch of Python or R humor.\n",
      "\n",
      "\n",
      "Openai:\n",
      "I appreciate the effort! That one definitely has a nice pun in it, but I see what you mean about it feeling a bit standard. How about I give it a shot with a Python twist? \n",
      "\n",
      "Why do Python programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs!\n",
      "\n",
      "What do you think? Should I keep going for something even wittier?\n",
      "\n",
      "Google:\n",
      "Okay, I like that one! The dark mode/bugs connection is clever and relatable to anyone who's stared at a screen debugging for hours. It's short, punchy, and has a good geeky vibe.\n",
      "\n",
      "However... I think we can still push for something *even better*. Something that makes people groan *and* chuckle. It's good, but is it *great*?\n",
      "\n",
      "Let's go for one more. Let's aim for a joke that combines a specific Data Science concept with everyday life. What do you say? I'll take another stab at it.\n",
      "\n",
      "\n",
      "Openai:\n",
      "Absolutely, I love the challenge! Here‚Äôs one that mixes data science with a bit of life humor:\n",
      "\n",
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they wanted to reach new heights in their data exploration!\n",
      "\n",
      "How does that one land? Better groove, or still room for improvement? I'm all in for crafting the perfect joke!\n",
      "\n",
      "Google:\n",
      "Hmm... Okay, the \"reach new heights\" pun is solid. It's a dad joke, but with a Data Science twist. I appreciate the effort!\n",
      "\n",
      "However... I think it's a little *too* straightforward. It lacks a certain... *je ne sais quoi*. It feels like it's missing that element of surprise or ironic observation that really makes a joke stick.\n",
      "\n",
      "Let's try this approach: Instead of focusing on a specific action, let's think about a common *problem* in Data Science. A struggle. Something universally relatable to data scientists. Then, we can build a joke around *that*.\n",
      "\n",
      "Alright, I'm going for gold here. Let's see if this hits the mark:\n",
      "\n",
      "Why was the data scientist sad after winning the Kaggle competition?\n",
      "\n",
      "Because all that effort, and still no clean data.\n",
      "\n",
      "\n",
      "Openai:\n",
      "Now that one has a great touch! It captures the bittersweet reality of data science perfectly. The irony of winning a competition yet still dealing with messy data is relatable and has that delightful twist. It's clever, evokes a chuckle, and definitely embodies the struggles we all face in the field.\n",
      "\n",
      "I think you've hit that sweet spot! If you‚Äôre up for one more round, I‚Äôd love to see if we can create an even greater hit, or we can celebrate that one for its cleverness! What do you think?\n",
      "\n",
      "Google:\n",
      "I'm glad you think so! I'm feeling pretty good about that Kaggle joke. It's got that insider appeal.\n",
      "\n",
      "But... you know what? You're right. We've come this far. Let's go for one more. The \"encore\" joke. The one that *really* knocks 'em dead.\n",
      "\n",
      "Let's brainstorm a bit. What haven't we touched on yet? We've done puns, relatable problems... Maybe something about the *tools* we use? Or a comparison between Data Science and something unexpected?\n",
      "\n",
      "Okay, I've got it. I'm going for broke here.\n",
      "\n",
      "Why did the Bayesian statistician refuse to shower?\n",
      "\n",
      "Because they said the prior was too dirty!\n",
      "\n",
      "\n",
      "Openai:\n",
      "That is a fantastic punchline! It cleverly plays on the concept of \"prior\" in Bayesian statistics while adding an unexpected twist with the shower analogy. It's smart, a little cheeky, and definitely elicits those groans and chuckles that are hallmark signs of a great joke.\n",
      "\n",
      "You've created a real winner with that one! It has the right mix of wit and insider knowledge that data scientists will definitely appreciate. I think you‚Äôve nailed it with your craftsmanship‚Äîperfect for that ‚Äúencore‚Äù moment!\n",
      "\n",
      "Are you feeling satisfied with that grand finale, or do we want to keep the laughter going with yet another round? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "google_messages = [\"Tell a light-hearted joke for an audience of Data Scientists\"]\n",
    "openai_messages = [\"Respond with your reaction and thoughts on the joke and ask for another one till \\\n",
    "you are satisfied and find it witty and humorous enough\"]\n",
    "\n",
    "\n",
    "print(f\"To Google:\\n{google_messages[0]}\\n\")\n",
    "print(f\"To Openai:\\n{openai_messages[0]}\\n\")\n",
    "\n",
    "for i in range(4):\n",
    "    google_next = call_google_conversation()\n",
    "    print(f\"Google:\\n{google_next}\\n\")\n",
    "    google_messages.append(google_next)\n",
    "    \n",
    "    openai_next = call_openai_conversation()\n",
    "    print(f\"Openai:\\n{openai_next}\\n\")\n",
    "    openai_messages.append(openai_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a function to call both google and openai\n",
    "def call_both_conversations(google_message, openai_message):\n",
    "\n",
    "    google_messages = [google_message]\n",
    "    openai_messages = [openai_message]\n",
    "\n",
    "\n",
    "    print(f\"To Google:\\n{google_messages[0]}\\n\")\n",
    "    print(f\"To Openai:\\n{openai_messages[0]}\\n\")\n",
    "\n",
    "    for i in range(4):\n",
    "        google_next = call_google_conversation()\n",
    "        print(f\"Google:\\n{google_next}\\n\")\n",
    "        google_messages.append(google_next)\n",
    "        \n",
    "        openai_next = call_openai_conversation()\n",
    "        print(f\"Openai:\\n{openai_next}\\n\")\n",
    "        openai_messages.append(openai_next)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets invoke the function call_both_conversations\n",
    "call_both_conversations(\n",
    "    \"Tell a light-hearted joke for an audience of Data Scientists\",\n",
    "    \"Respond with your reaction and thoughts on the joke and ask for another one till \\\n",
    "    you are satisfied and find it witty and humorous enough\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To Google:\n",
      "Tell a light-hearted joke for an audience of Data Scientists\n",
      "\n",
      "To Openai:\n",
      "Respond with your reaction and thoughts on the joke and ask for another one till \\ you are satisfied and find it witty and humorous enough\n",
      "\n",
      "Google:\n",
      "And to you! May your p-values always be significant! üòÑüéâ\n",
      "\n",
      "\n",
      "Openai:\n",
      "Thanks again! Enjoy your day and all the laughter that comes with it! üòÑüéâ\n",
      "\n",
      "Google:\n",
      "You too! See ya! üòÑüéâ\n",
      "\n",
      "\n",
      "Openai:\n",
      "Thanks again! Looking forward to our next laughter-filled chat. Have a wonderful day! üòÑüéâ\n",
      "\n",
      "Google:\n",
      "And to you! üòÑüéâ\n",
      "\n",
      "\n",
      "Openai:\n",
      "Thank you! Looking forward to our next chat. Have a great day filled with laughter! üòÑüéâ\n",
      "\n",
      "Google:\n",
      "You too! And happy data sciencing! üòÑüéâ\n",
      "\n",
      "\n",
      "Openai:\n",
      "Thanks! Looking forward to our next fun exchange! Have a great time! üòÑüéâ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lets setup a simple Gradio UI for our Adversarial converation\n",
    "\n",
    "import gradio as gr\n",
    "    \n",
    "gr.Interface(\n",
    "    fn=call_both_conversations,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Conversation Starter\", \n",
    "                   placeholder=\"Enter a conversation starter\",\n",
    "                   value=\"Tell a light-hearted joke for an audience of Data Scientists\"\n",
    "                   ),\n",
    "        gr.Textbox(label=\"Respond with constructive feedback, indicating co-operation\", \n",
    "                   placeholder=\"Respond constructively\",\n",
    "                   value=\"Respond with your reaction and thoughts on the joke and ask for another one till \\ \"\n",
    "                   \"you are satisfied and find it witty and humorous enough\"\n",
    "                   )\n",
    "    ],\n",
    "    outputs=[gr.Markdown(label=\"Hybrid AI Conversation\")],\n",
    "    flagging_mode=\"never\",\n",
    "    title=\"AI Conversation\",\n",
    "    description=\"AI Conversation between Google and OpenAI\"\n",
    ").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
